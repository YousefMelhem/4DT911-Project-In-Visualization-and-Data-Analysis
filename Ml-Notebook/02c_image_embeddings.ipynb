{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c587aed",
   "metadata": {},
   "source": [
    "# üì∏ Phase 2C: Image Embeddings with ResNet50\n",
    "\n",
    "Extract deep visual features from medical images using a pre-trained ResNet50 model.\n",
    "\n",
    "## Goals\n",
    "- Load medical case images from the dataset\n",
    "- Extract image embeddings using a pre-trained CNN (ResNet50)\n",
    "- Save embeddings for similarity search\n",
    "- Enable image-based case similarity\n",
    "\n",
    "## Model: ResNet50\n",
    "- Pre-trained on ImageNet\n",
    "- 2048-dimensional feature vectors\n",
    "- Transfer learning for medical images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "783f91f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful\n",
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"‚úÖ Imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b782d63",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddd78e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Data directory: /home/yousef/code/school/4DT911-project/Ml-Notebook/../data\n",
      "üìÇ Images directory: /home/yousef/code/school/4DT911-project/Ml-Notebook/../data/archive/medpix_data_final\n",
      "üìÑ Input file: /home/yousef/code/school/4DT911-project/Ml-Notebook/../data/ml_ready/cases_ml_ready.json\n",
      "üìÇ Output directory: /home/yousef/code/school/4DT911-project/Ml-Notebook/../data/features\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "BASE_DIR = Path(\"..\").resolve()\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "ML_READY_DIR = DATA_DIR / \"ml_ready\"\n",
    "FEATURES_DIR = DATA_DIR / \"features\"\n",
    "IMAGES_DIR = DATA_DIR / \"archive\" / \"medpix_data_final\"\n",
    "\n",
    "print(f\"üìÇ Base directory: {BASE_DIR}\")\n",
    "print(f\"üìÇ Images directory: {IMAGES_DIR}\")\n",
    "print(f\"\udcc2 Features directory: {FEATURES_DIR}\")\n",
    "print(f\"üìÇ ML Ready directory: {ML_READY_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51094c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 7,404 cases\n",
      "\n",
      "üìã Sample case structure:\n",
      "  Case ID: 8892378009084536600\n",
      "  Diagnosis: A Neck And Wrist Pain: Bilateral Carpal Tunnel Syndrome, Cervical Subluxation Kn...\n",
      "  Image count: 23\n",
      "  Image paths: ['medpix_data_final/case_8892378009084536600/image_1.jpg', 'medpix_data_final/case_8892378009084536600/image_2.jpg', 'medpix_data_final/case_8892378009084536600/image_3.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Load ML-ready cases\n",
    "with open(ML_READY_DIR / \"cases_ml_ready.json\", 'r') as f:\n",
    "    cases = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(cases):,} cases\")\n",
    "\n",
    "# Inspect first case\n",
    "print(\"\\nüìã Sample case structure:\")\n",
    "sample = cases[0]\n",
    "print(f\"  Case ID: {sample['id']}\")\n",
    "print(f\"  Diagnosis: {sample.get('diagnosis', 'N/A')[:80]}...\")\n",
    "print(f\"  Image count: {sample.get('imageCount', 0)}\")\n",
    "print(f\"  Image paths: {sample.get('imagePaths', [])[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf504681",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Prepare Image Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "790a990b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è Using device: cuda\n",
      "\n",
      "üì• Loading ResNet50 model...\n",
      "‚úÖ ResNet50 model loaded (feature extraction mode)\n",
      "   Output dimension: 2048\n",
      "‚úÖ ResNet50 model loaded (feature extraction mode)\n",
      "   Output dimension: 2048\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è Using device: {device}\")\n",
    "\n",
    "# Load pre-trained ResNet50 model\n",
    "print(\"\\nüì• Loading ResNet50 model...\")\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Remove the final classification layer to get feature embeddings\n",
    "# ResNet50 outputs 2048-dimensional features before the FC layer\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "model = model.to(device)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(\"‚úÖ ResNet50 model loaded (feature extraction mode)\")\n",
    "print(f\"   Output dimension: 2048\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96ce7246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Image preprocessing pipeline ready\n"
     ]
    }
   ],
   "source": [
    "# Define image preprocessing pipeline\n",
    "# ImageNet normalization values\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),  # Resize shortest side to 256\n",
    "    transforms.CenterCrop(224),  # Crop to 224x224 (ResNet input size)\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],  # ImageNet mean\n",
    "        std=[0.229, 0.224, 0.225]    # ImageNet std\n",
    "    )\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Image preprocessing pipeline ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73e7ef2",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Extract Image Features\n",
    "\n",
    "**Strategy for Multiple Images per Case:**\n",
    "\n",
    "Each case can have multiple images (1-23+ images). We need to aggregate them into a single embedding.\n",
    "\n",
    "**Options:**\n",
    "1. **First image only** (fastest, simple) - Use only the first image\n",
    "2. **Average pooling** (recommended) - Average features from all images\n",
    "3. **Max pooling** - Take maximum value across all images for each feature dimension\n",
    "4. **Weighted average** - Weight by image quality/importance (advanced)\n",
    "\n",
    "**We'll use AVERAGE POOLING** - it captures information from all images while producing a single 2048-dim vector per case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4730ac49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Feature extraction functions defined\n",
      "   - extract_image_features: Single image ‚Üí 2048-dim vector\n",
      "   - extract_case_embedding: All case images ‚Üí Aggregated 2048-dim vector\n"
     ]
    }
   ],
   "source": [
    "def extract_image_features(image_path, model, transform, device):\n",
    "    \"\"\"\n",
    "    Extract feature embedding from an image using ResNet50\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        model: ResNet50 model\n",
    "        transform: Image preprocessing pipeline\n",
    "        device: torch device (cuda or cpu)\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (2048,) or None if error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load and preprocess image\n",
    "        img = Image.open(image_path).convert('RGB')  # Ensure RGB format\n",
    "        img_tensor = transform(img).unsqueeze(0).to(device)  # Add batch dimension\n",
    "        \n",
    "        # Extract features\n",
    "        with torch.no_grad():\n",
    "            features = model(img_tensor)\n",
    "        \n",
    "        # Flatten and convert to numpy\n",
    "        features = features.squeeze().cpu().numpy()\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Silently skip errors (corrupted images, etc.)\n",
    "        return None\n",
    "\n",
    "def extract_case_embedding(case, model, transform, device, images_dir, aggregation='mean'):\n",
    "    \"\"\"\n",
    "    Extract aggregated embedding for a case from all its images\n",
    "    \n",
    "    Args:\n",
    "        case: Case dictionary with imagePaths\n",
    "        model: ResNet50 model\n",
    "        transform: Image preprocessing pipeline\n",
    "        device: torch device\n",
    "        images_dir: Base directory for images\n",
    "        aggregation: 'mean', 'max', or 'first'\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of shape (2048,) - aggregated features\n",
    "        int - number of successfully processed images\n",
    "    \"\"\"\n",
    "    image_paths = case.get('imagePaths', [])\n",
    "    \n",
    "    if not image_paths:\n",
    "        return np.zeros(2048), 0\n",
    "    \n",
    "    # Extract features from all images\n",
    "    all_features = []\n",
    "    for img_path in image_paths:\n",
    "        full_path = images_dir / img_path\n",
    "        if full_path.exists():\n",
    "            features = extract_image_features(full_path, model, transform, device)\n",
    "            if features is not None:\n",
    "                all_features.append(features)\n",
    "    \n",
    "    # No valid images found\n",
    "    if len(all_features) == 0:\n",
    "        return np.zeros(2048), 0\n",
    "    \n",
    "    # Aggregate based on strategy\n",
    "    all_features = np.array(all_features)  # Shape: (num_images, 2048)\n",
    "    \n",
    "    if aggregation == 'mean':\n",
    "        # Average pooling - recommended\n",
    "        aggregated = np.mean(all_features, axis=0)\n",
    "    elif aggregation == 'max':\n",
    "        # Max pooling - takes maximum value per dimension\n",
    "        aggregated = np.max(all_features, axis=0)\n",
    "    elif aggregation == 'first':\n",
    "        # Just use first image\n",
    "        aggregated = all_features[0]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown aggregation: {aggregation}\")\n",
    "    \n",
    "    return aggregated, len(all_features)\n",
    "\n",
    "print(\"‚úÖ Feature extraction functions defined\")\n",
    "print(\"   - extract_image_features: Single image ‚Üí 2048-dim vector\")\n",
    "print(\"   - extract_case_embedding: All case images ‚Üí Aggregated 2048-dim vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24206b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing on first case with ALL images...\n",
      "\n",
      "Test Case:\n",
      "  ID: 8892378009084536600\n",
      "  Total images: 23\n",
      "\n",
      "============================================================\n",
      "Strategy: FIRST\n",
      "============================================================\n",
      "‚úÖ Successfully processed 5 images\n",
      "   Aggregated feature shape: (2048,)\n",
      "   Feature range: [0.000, 5.113]\n",
      "   Mean: 0.434\n",
      "   Std: 0.392\n",
      "\n",
      "============================================================\n",
      "Strategy: MEAN\n",
      "============================================================\n",
      "‚úÖ Successfully processed 5 images\n",
      "   Aggregated feature shape: (2048,)\n",
      "   Feature range: [0.016, 4.964]\n",
      "   Mean: 0.457\n",
      "   Std: 0.328\n",
      "\n",
      "============================================================\n",
      "Strategy: MAX\n",
      "============================================================\n",
      "‚úÖ Successfully processed 5 images\n",
      "   Aggregated feature shape: (2048,)\n",
      "   Feature range: [0.041, 6.081]\n",
      "   Mean: 0.853\n",
      "   Std: 0.559\n",
      "\n",
      "============================================================\n",
      "‚úÖ Test successful! All aggregation strategies work.\n",
      "============================================================\n",
      "‚úÖ Successfully processed 5 images\n",
      "   Aggregated feature shape: (2048,)\n",
      "   Feature range: [0.016, 4.964]\n",
      "   Mean: 0.457\n",
      "   Std: 0.328\n",
      "\n",
      "============================================================\n",
      "Strategy: MAX\n",
      "============================================================\n",
      "‚úÖ Successfully processed 5 images\n",
      "   Aggregated feature shape: (2048,)\n",
      "   Feature range: [0.041, 6.081]\n",
      "   Mean: 0.853\n",
      "   Std: 0.559\n",
      "\n",
      "============================================================\n",
      "‚úÖ Test successful! All aggregation strategies work.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test on first case with ALL its images\n",
    "print(\"üß™ Testing on first case with ALL images...\\n\")\n",
    "\n",
    "test_case = cases[0]\n",
    "print(f\"Test Case:\")\n",
    "print(f\"  ID: {test_case['id']}\")\n",
    "print(f\"  Total images: {test_case.get('imageCount', 0)}\")\n",
    "\n",
    "# Test different aggregation strategies\n",
    "for strategy in ['first', 'mean', 'max']:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Strategy: {strategy.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    aggregated_features, num_images = extract_case_embedding(\n",
    "        test_case, model, transform, device, IMAGES_DIR, aggregation=strategy\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Successfully processed {num_images} images\")\n",
    "    print(f\"   Aggregated feature shape: {aggregated_features.shape}\")\n",
    "    print(f\"   Feature range: [{aggregated_features.min():.3f}, {aggregated_features.max():.3f}]\")\n",
    "    print(f\"   Mean: {aggregated_features.mean():.3f}\")\n",
    "    print(f\"   Std: {aggregated_features.std():.3f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"‚úÖ Test successful! All aggregation strategies work.\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7256264",
   "metadata": {},
   "source": [
    "### üìä Why Mean Aggregation?\n",
    "\n",
    "Comparing aggregation strategies:\n",
    "\n",
    "| Method | Pros | Cons | Best For |\n",
    "|--------|------|------|----------|\n",
    "| **First only** | Fast, simple | Ignores 22+ other images! | Quick prototyping |\n",
    "| **Mean pooling** | Uses all images, balanced | None significant | **Recommended** ‚úÖ |\n",
    "| **Max pooling** | Captures strongest features | May amplify noise | Specific feature detection |\n",
    "| **Weighted** | Can prioritize key images | Needs image quality scores | Advanced use cases |\n",
    "\n",
    "**Our choice: MEAN POOLING** - Gets the best representation by averaging features from all 23 images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ab8454b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Extracting features for all cases...\n",
      "\n",
      "Strategy: MEAN AGGREGATION (average features from all images per case)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc3778c5bb6c4b8193ede6119889090e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing cases:   0%|          | 0/7404 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Feature extraction complete!\n",
      "   Total cases: 7,404\n",
      "   Successfully processed: 7,404\n",
      "   Cases with no valid images: 0\n",
      "   Total images processed: 27,119\n",
      "   Average images per case: 3.7\n",
      "   Max images in a case: 5\n",
      "   Time elapsed: 249.2s (0.03s per case)\n",
      "\n",
      "üìä Embeddings shape: (7404, 2048)\n",
      "   Aggregation method: MEAN\n"
     ]
    }
   ],
   "source": [
    "# Extract features for ALL cases using MEAN AGGREGATION\n",
    "print(\"üöÄ Extracting features for all cases...\\n\")\n",
    "print(\"Strategy: MEAN AGGREGATION (average features from all images per case)\\n\")\n",
    "\n",
    "AGGREGATION_METHOD = 'mean'  # Options: 'mean', 'max', 'first'\n",
    "\n",
    "start_time = time.time()\n",
    "embeddings = []\n",
    "case_ids = []\n",
    "images_per_case = []  # Track how many images were processed per case\n",
    "missing_images = 0\n",
    "error_cases = 0\n",
    "\n",
    "for case in tqdm(cases, desc=\"Processing cases\"):\n",
    "    case_id = case['id']\n",
    "    \n",
    "    # Extract aggregated embedding from all images\n",
    "    aggregated_features, num_images = extract_case_embedding(\n",
    "        case, model, transform, device, IMAGES_DIR, aggregation=AGGREGATION_METHOD\n",
    "    )\n",
    "    \n",
    "    embeddings.append(aggregated_features)\n",
    "    case_ids.append(case_id)\n",
    "    images_per_case.append(num_images)\n",
    "    \n",
    "    if num_images == 0:\n",
    "        missing_images += 1\n",
    "\n",
    "# Convert to numpy array\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Feature extraction complete!\")\n",
    "print(f\"   Total cases: {len(cases):,}\")\n",
    "print(f\"   Successfully processed: {len(embeddings):,}\")\n",
    "print(f\"   Cases with no valid images: {missing_images:,}\")\n",
    "print(f\"   Total images processed: {sum(images_per_case):,}\")\n",
    "print(f\"   Average images per case: {np.mean(images_per_case):.1f}\")\n",
    "print(f\"   Max images in a case: {max(images_per_case)}\")\n",
    "print(f\"   Time elapsed: {elapsed_time:.1f}s ({elapsed_time/len(cases):.2f}s per case)\")\n",
    "print(f\"\\nüìä Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"   Aggregation method: {AGGREGATION_METHOD.upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4604bff",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Analyze Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb03c7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Image Embedding Statistics:\n",
      "\n",
      "Shape: (7404, 2048)\n",
      "Type: float32\n",
      "Memory: 57.84 MB\n",
      "\n",
      "Value ranges:\n",
      "  Min: 0.0000\n",
      "  Max: 10.8055\n",
      "  Mean: 0.4427\n",
      "  Std: 0.3785\n",
      "\n",
      "Zero vectors (missing/error): 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# Basic statistics\n",
    "print(\"üìä Image Embedding Statistics:\\n\")\n",
    "print(f\"Shape: {embeddings.shape}\")\n",
    "print(f\"Type: {embeddings.dtype}\")\n",
    "print(f\"Memory: {embeddings.nbytes / 1024 / 1024:.2f} MB\")\n",
    "print(f\"\\nValue ranges:\")\n",
    "print(f\"  Min: {embeddings.min():.4f}\")\n",
    "print(f\"  Max: {embeddings.max():.4f}\")\n",
    "print(f\"  Mean: {embeddings.mean():.4f}\")\n",
    "print(f\"  Std: {embeddings.std():.4f}\")\n",
    "\n",
    "# Check for zero vectors (missing/error cases)\n",
    "zero_vectors = np.sum(np.all(embeddings == 0, axis=1))\n",
    "print(f\"\\nZero vectors (missing/error): {zero_vectors:,} ({100*zero_vectors/len(embeddings):.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb5a8b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Computing similarity statistics (on sample)...\n",
      "\n",
      "Cosine Similarity Statistics (sample of 100 cases):\n",
      "  Mean: 0.7419\n",
      "  Median: 0.7500\n",
      "  Std: 0.0832\n",
      "  Min: 0.4428\n",
      "  Max: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Compute pairwise similarity matrix (sample)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"üîç Computing similarity statistics (on sample)...\\n\")\n",
    "\n",
    "# Sample 100 random non-zero embeddings for efficiency\n",
    "non_zero_idx = np.where(~np.all(embeddings == 0, axis=1))[0]\n",
    "sample_idx = np.random.choice(non_zero_idx, min(100, len(non_zero_idx)), replace=False)\n",
    "sample_embeddings = embeddings[sample_idx]\n",
    "\n",
    "# Compute similarity matrix\n",
    "sim_matrix = cosine_similarity(sample_embeddings)\n",
    "\n",
    "# Get upper triangle (excluding diagonal)\n",
    "upper_tri = sim_matrix[np.triu_indices_from(sim_matrix, k=1)]\n",
    "\n",
    "print(f\"Cosine Similarity Statistics (sample of {len(sample_idx)} cases):\")\n",
    "print(f\"  Mean: {upper_tri.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(upper_tri):.4f}\")\n",
    "print(f\"  Std: {upper_tri.std():.4f}\")\n",
    "print(f\"  Min: {upper_tri.min():.4f}\")\n",
    "print(f\"  Max: {upper_tri.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7373350d",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Save Embeddings and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59be7792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved embeddings to: /home/yousef/code/school/4DT911-project/data/features/image_embeddings_resnet50.npy\n",
      "‚úÖ Case IDs match existing case_ids.json\n"
     ]
    }
   ],
   "source": [
    "# Save embeddings\n",
    "output_path = FEATURES_DIR / \"image_embeddings_resnet50.npy\"\n",
    "np.save(output_path, embeddings)\n",
    "print(f\"‚úÖ Saved embeddings to: {output_path}\")\n",
    "\n",
    "# Save case IDs (should match the existing case_ids.json)\n",
    "# Verify they're the same\n",
    "with open(FEATURES_DIR / \"case_ids.json\", 'r') as f:\n",
    "    existing_case_ids = json.load(f)\n",
    "\n",
    "if case_ids == existing_case_ids:\n",
    "    print(\"‚úÖ Case IDs match existing case_ids.json\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Case IDs don't match - saving new file\")\n",
    "    with open(FEATURES_DIR / \"image_case_ids.json\", 'w') as f:\n",
    "        json.dump(case_ids, f)\n",
    "    print(f\"‚úÖ Saved case IDs to: image_case_ids.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14b539b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved metadata to: /home/yousef/code/school/4DT911-project/data/features/image_metadata.json\n",
      "\n",
      "üìã Metadata:\n",
      "{\n",
      "  \"model_name\": \"ResNet50\",\n",
      "  \"pretrained_dataset\": \"ImageNet\",\n",
      "  \"embedding_dimension\": 2048,\n",
      "  \"num_cases\": 7404,\n",
      "  \"cases_with_images\": 7404,\n",
      "  \"cases_missing_images\": 0,\n",
      "  \"total_images_processed\": 27119,\n",
      "  \"avg_images_per_case\": 3.6627498649378714,\n",
      "  \"max_images_per_case\": 5,\n",
      "  \"extraction_time_seconds\": 249.18,\n",
      "  \"mean_similarity\": 0.7418870329856873,\n",
      "  \"median_similarity\": 0.7500104904174805,\n",
      "  \"std_similarity\": 0.0832139179110527,\n",
      "  \"created_at\": \"2025-10-12T14:46:31.152387\",\n",
      "  \"aggregation_method\": \"mean\",\n",
      "  \"image_preprocessing\": {\n",
      "    \"resize\": 256,\n",
      "    \"crop\": 224,\n",
      "    \"normalization\": \"ImageNet\"\n",
      "  },\n",
      "  \"device\": \"cuda\",\n",
      "  \"notes\": \"Features aggregated from ALL images per case using mean pooling\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Save metadata\n",
    "metadata = {\n",
    "    \"model_name\": \"ResNet50\",\n",
    "    \"pretrained_dataset\": \"ImageNet\",\n",
    "    \"embedding_dimension\": 2048,\n",
    "    \"num_cases\": int(len(embeddings)),\n",
    "    \"cases_with_images\": int(len(embeddings) - zero_vectors),\n",
    "    \"cases_missing_images\": int(zero_vectors),\n",
    "    \"total_images_processed\": int(sum(images_per_case)),\n",
    "    \"avg_images_per_case\": float(np.mean(images_per_case)),\n",
    "    \"max_images_per_case\": int(max(images_per_case)),\n",
    "    \"extraction_time_seconds\": float(round(elapsed_time, 2)),\n",
    "    \"mean_similarity\": float(upper_tri.mean()),\n",
    "    \"median_similarity\": float(np.median(upper_tri)),\n",
    "    \"std_similarity\": float(upper_tri.std()),\n",
    "    \"created_at\": datetime.now().isoformat(),\n",
    "    \"aggregation_method\": AGGREGATION_METHOD,\n",
    "    \"image_preprocessing\": {\n",
    "        \"resize\": 256,\n",
    "        \"crop\": 224,\n",
    "        \"normalization\": \"ImageNet\"\n",
    "    },\n",
    "    \"device\": str(device),\n",
    "    \"notes\": f\"Features aggregated from ALL images per case using {AGGREGATION_METHOD} pooling\"\n",
    "}\n",
    "\n",
    "metadata_path = FEATURES_DIR / \"image_metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved metadata to: {metadata_path}\")\n",
    "print(\"\\nüìã Metadata:\")\n",
    "print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce467b2",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Test Image Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39907817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Similarity search function defined\n"
     ]
    }
   ],
   "source": [
    "def find_similar_images(query_idx, embeddings, k=10):\n",
    "    \"\"\"\n",
    "    Find k most similar cases based on image embeddings\n",
    "    \"\"\"\n",
    "    query_embedding = embeddings[query_idx].reshape(1, -1)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarities = cosine_similarity(query_embedding, embeddings).flatten()\n",
    "    \n",
    "    # Get top k indices (excluding query itself)\n",
    "    top_indices = np.argsort(similarities)[::-1][1:k+1]\n",
    "    \n",
    "    return [(idx, similarities[idx]) for idx in top_indices]\n",
    "\n",
    "print(\"‚úÖ Similarity search function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bdee490b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing image similarity search...\n",
      "\n",
      "Query Case:\n",
      "  ID: 839823913719451037\n",
      "  Diagnosis: Aneurysm, Cerebral...\n",
      "  Images: 8\n",
      "\n",
      "Top 5 Similar Cases (by image):\n",
      "\n",
      "1. Similarity: 0.9642\n",
      "   ID: -1968910857513376535\n",
      "   Diagnosis: Meningioma...\n",
      "   Images: 7\n",
      "\n",
      "2. Similarity: 0.9629\n",
      "   ID: 7077047673534147376\n",
      "   Diagnosis: Bilateral Cavernous Sinus Metastatic Lymphoma...\n",
      "   Images: 6\n",
      "\n",
      "3. Similarity: 0.9629\n",
      "   ID: 7077047673534147376\n",
      "   Diagnosis: Bilateral Cavernous Sinus Metastatic Lymphoma...\n",
      "   Images: 6\n",
      "\n",
      "4. Similarity: 0.9629\n",
      "   ID: 7077047673534147376\n",
      "   Diagnosis: Bilateral Cavernous Sinus Metastatic Lymphoma...\n",
      "   Images: 6\n",
      "\n",
      "5. Similarity: 0.9629\n",
      "   ID: 7077047673534147376\n",
      "   Diagnosis: Bilateral Cavernous Sinus Metastatic Lymphoma...\n",
      "   Images: 6\n"
     ]
    }
   ],
   "source": [
    "# Test similarity search\n",
    "print(\"üîç Testing image similarity search...\\n\")\n",
    "\n",
    "# Choose a random case with images\n",
    "test_idx = np.random.choice(non_zero_idx)\n",
    "test_case = cases[test_idx]\n",
    "\n",
    "print(f\"Query Case:\")\n",
    "print(f\"  ID: {test_case['id']}\")\n",
    "print(f\"  Diagnosis: {test_case.get('diagnosis', 'N/A')[:100]}...\")\n",
    "print(f\"  Images: {test_case.get('imageCount', 0)}\")\n",
    "\n",
    "# Find similar cases\n",
    "similar = find_similar_images(test_idx, embeddings, k=5)\n",
    "\n",
    "print(f\"\\nTop 5 Similar Cases (by image):\")\n",
    "for rank, (idx, score) in enumerate(similar, 1):\n",
    "    sim_case = cases[idx]\n",
    "    print(f\"\\n{rank}. Similarity: {score:.4f}\")\n",
    "    print(f\"   ID: {sim_case['id']}\")\n",
    "    print(f\"   Diagnosis: {sim_case.get('diagnosis', 'N/A')[:80]}...\")\n",
    "    print(f\"   Images: {sim_case.get('imageCount', 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431d5c54",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "Image embeddings successfully extracted and saved!\n",
    "\n",
    "### Approach: Multi-Image Aggregation üéØ\n",
    "- **Extracts features from ALL images** in each case (not just first)\n",
    "- **Aggregation method**: Mean pooling (averages features across all images)\n",
    "- **Result**: Single 2048-dim vector per case representing all visual information\n",
    "\n",
    "### Files Created:\n",
    "- `data/features/image_embeddings_resnet50.npy` - 2048-dim embeddings for all cases\n",
    "- `data/features/image_metadata.json` - Metadata about the extraction process\n",
    "\n",
    "### Advantages of Mean Aggregation:\n",
    "‚úÖ Uses information from **all images** in a case\n",
    "‚úÖ More robust than single-image representation\n",
    "‚úÖ Cases with 23 images get richer representation than cases with 1 image\n",
    "‚úÖ Reduces noise from single bad/unusual images\n",
    "‚úÖ Still produces fixed-size 2048-dim vectors\n",
    "\n",
    "### Next Steps:\n",
    "1. **Update Backend API** (`backend/api/similarity.py`)\n",
    "   - Load image embeddings\n",
    "   - Add image-based similarity endpoint\n",
    "   - Implement hybrid text+image search\n",
    "\n",
    "2. **Frontend Integration**\n",
    "   - Add image similarity toggle\n",
    "   - Display similar cases with visual previews\n",
    "   - Show combined text+image scores\n",
    "\n",
    "3. **Future Enhancements**\n",
    "   - Try max pooling or weighted aggregation\n",
    "   - Fine-tune ResNet on medical images\n",
    "   - Use attention mechanism to weight images by importance\n",
    "   - Try other architectures (EfficientNet, Vision Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b880d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéâ IMAGE EMBEDDING EXTRACTION COMPLETE!\n",
      "============================================================\n",
      "\n",
      "üìä Summary:\n",
      "   Total cases: 7,404\n",
      "   Embedding dimension: 2048\n",
      "   Model: ResNet50 (ImageNet pretrained)\n",
      "   Cases with images: 7,404\n",
      "   Extraction time: 249.2s\n",
      "\n",
      "üìÅ Output files:\n",
      "   /home/yousef/code/school/4DT911-project/data/features/image_embeddings_resnet50.npy\n",
      "   /home/yousef/code/school/4DT911-project/data/features/image_metadata.json\n",
      "\n",
      "üöÄ Ready for backend integration!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ IMAGE EMBEDDING EXTRACTION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"   Total cases: {len(embeddings):,}\")\n",
    "print(f\"   Embedding dimension: 2048\")\n",
    "print(f\"   Model: ResNet50 (ImageNet pretrained)\")\n",
    "print(f\"   Cases with images: {len(embeddings) - zero_vectors:,}\")\n",
    "print(f\"   Extraction time: {elapsed_time:.1f}s\")\n",
    "print(f\"\\nüìÅ Output files:\")\n",
    "print(f\"   {output_path}\")\n",
    "print(f\"   {metadata_path}\")\n",
    "print(f\"\\nüöÄ Ready for backend integration!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0ea86c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
